{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simsIab\\anaconda3\\envs\\bertClassfication\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from rich.pretty import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data') == False:\n",
    "    os.makedirs('data')\n",
    "\n",
    "if os.path.exists('model') == False:\n",
    "    os.makedirs('model')\n",
    "\n",
    "if os.path.exists('result_img') == False:\n",
    "    os.makedirs('result_img')\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"num_class\": 4,\n",
    "    \"time\": str(datetime.now().strftime(\"%m-%d-%H-%M\")),\n",
    "    \"seed\": 42,\n",
    "    \"train_size\": 3000,\n",
    "    \"val_size\": 500,\n",
    "    # Hyperparameters\n",
    "    \"model_name\": 'BERT', # If U have a lot of different models, it is easy for U to know what it is\n",
    "    \"config\": 'nbroad/ESG-BERT', # which pre-trained model config U use\n",
    "    \"learning_rate\": 1e-4, # the speed that model learn\n",
    "    \"epochs\": 3, # If U would fine-tune it, the epochs didn't need to set too much\n",
    "    \"max_len\": 512, # the max length of input tokens in the BERT model\n",
    "    \"batch_size\": 64, \n",
    "    \"dropout\": 0.1, # how random amount will be give up\n",
    "    \"activation\": 'tanh',\n",
    "    \"hidden_dim\": 384,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入資料集\n",
    "dataset = load_dataset(\"FinanceMTEB/ESG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span>train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label_text'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3000</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span>test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label_text'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0mtrain: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0mfeatures: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m, \u001b[32m'label_text'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0mnum_rows: \u001b[1;36m3000\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0mtest: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0mfeatures: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m, \u001b[32m'label_text'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0mnum_rows: \u001b[1;36m1000\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This explains the increase in high-risk events in 2017, and the subsequent improvement in 2018 and 2019, as the action items implemented supported the decreasing number of high-risk incidents.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'label_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'social'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m'This explains the increase in high-risk events in 2017, and the subsequent improvement in 2018 and 2019, as the action items implemented supported the decreasing number of high-risk incidents.'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'label'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'label_text'\u001b[0m: \u001b[32m'social'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(dataset)\n",
    "pprint(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This explains the increase in high-risk events...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focusing on shared value creation in 2019 allo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We included information on our policy and lobb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As of December 31, 2011, the project was appro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Independent lines of evidence continue to indi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  This explains the increase in high-risk events...      3\n",
       "1  Focusing on shared value creation in 2019 allo...      3\n",
       "2  We included information on our policy and lobb...      1\n",
       "3  As of December 31, 2011, the project was appro...      2\n",
       "4  Independent lines of evidence continue to indi...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將資料集轉為 DataFrame 的格式\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for data in dataset['train']:\n",
    "  train_data.append({'text':data['text'], 'label':data['label']})\n",
    "for data in dataset['test']:\n",
    "  test_data.append({'text':data['text'], 'label':data['label']})\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "test_df = pd.DataFrame(test_data, columns=['text', 'label'])\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    0.501000\n",
       "3    0.267333\n",
       "0    0.203000\n",
       "1    0.028667\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢查各種 label 比例是否有資料不平衡問題\n",
    "\n",
    "train_df.label.value_counts() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train_df: 3000\n",
      "# of val_df: 500\n",
      "# of test_df data: 500\n"
     ]
    }
   ],
   "source": [
    "# random_state 是固定資料 random 的結果，才不會每次切出來的資料集不一樣\n",
    "# train_size：指定 output 中前者資料數量占比 \n",
    "val_df, test_df = train_test_split(test_df, random_state=parameters['seed'], train_size=0.5)\n",
    "print('# of train_df:', len(train_df))\n",
    "print('# of val_df:', len(val_df))\n",
    "print('# of test_df data:', len(test_df))\n",
    "\n",
    "# save data\n",
    "# 這裡指定sep='\\t'，且不儲存DataFrame前面的index\n",
    "train_df.to_csv('./data/train.tsv', sep='\\t', index=False)\n",
    "val_df.to_csv('./data/val.tsv', sep='\\t', index=False)\n",
    "test_df.to_csv('./data/test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你發現在使用時常常有一長串的 warning 跳出來，可以用這行指令把它關掉\n",
    "# transformers.logging.set_verbosity_error() # Close the warning message\n",
    "\n",
    "config_name = 'nbroad/ESG-BERT' # 使用 ESG-BERT\n",
    "# .from_pretrained() 就是用現有的模型繼續做\n",
    "tokenizer = AutoTokenizer.from_pretrained(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢查 tokenizer 的 max_length\n",
    "max_length = tokenizer.model_max_length\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢查 model 的 hidden_size\n",
    "config = AutoConfig.from_pretrained(config_name)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0, 0, 1, 0],[0, 0, 1, 0],[0, 0, 1, 0]]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作 Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mode, df, specify, args):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        self.mode = mode                                                    # 指定為 \"train\", \"val\", \"test\"\n",
    "        self.df = df                                                        # 指定資料集\n",
    "        self.specify = specify                                              # 要進行 tokenize 的欄位名稱\n",
    "        if self.mode != 'test':\n",
    "          self.label = df['label']                                          # 當資料為 \"train\"、\"val\" 時，進行 label\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args[\"config\"])      # 指定 tokenize 的方法\n",
    "        self.max_len = args[\"max_len\"]                                      # 指定 tokenize 的最大長度\n",
    "        self.num_class = args[\"num_class\"]                                  # 指定 label 有幾種類別\n",
    "        \n",
    "    # 回傳 Dataset 的資料筆數\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # 當 self.num_class 大於2時，對 label 進行 one hot encodding\n",
    "    # 例如 label 為 2 ， num_classes = 4 時\n",
    "    # 則回傳 torch.tensor([0, 0, 1, 0]) ，其維度為 torch.size([num_classes])\n",
    "    def one_hot_label(self, label):\n",
    "        return Fun.one_hot(torch.tensor(label), num_classes = self.num_class)\n",
    "    \n",
    "    # 進行 tokenize\n",
    "    def tokenize(self, input_text):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,                     # 指定文本\n",
    "            max_length = self.max_len,      # 指定最長文本長度\n",
    "            truncation = True,              # 開啟截斷功能\n",
    "            padding = 'max_length'          # 依照 max_length 進行 padding\n",
    "        )\n",
    "        ids = inputs['input_ids']                 # size(self.max_len)\n",
    "        mask = inputs['attention_mask']           # size(self.max_len)\n",
    "        token_type_ids = inputs[\"token_type_ids\"] # size(self.max_len)\n",
    "        \n",
    "        return ids, mask, token_type_ids\n",
    "\n",
    "    # 獲得單一一筆資料\n",
    "    def __getitem__(self, index):\n",
    "        sentence = str(self.df[self.specify][index])            # 取出單筆資料的字串\n",
    "        ids, mask, token_type_ids = self.tokenize(sentence)     # 進行 tokenize\n",
    "\n",
    "        if self.mode == \"test\":\n",
    "            # 回傳 input_ids, attention_mask, totken_type_ids\n",
    "            # 需回傳 tensor 型態，其每個維度為 torch.Size([self.max_len])\n",
    "            return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
    "                torch.tensor(token_type_ids, dtype=torch.long)\n",
    "        else:\n",
    "            if self.num_class > 2:     # 如 self.num_class > 2 時，進行 one hot encodding\n",
    "            # 回傳 input_ids, attention_mask, totken_type_ids, labels\n",
    "            # 需回傳 tensor 型態，其前三者維度為 torch.Size([self.max_len])，最後者維度為 torch.Size([self.num_class])\n",
    "              return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
    "                torch.tensor(token_type_ids, dtype=torch.long), self.one_hot_label(self.label[index])\n",
    "            else:\n",
    "            # 回傳 input_ids, attention_mask, totken_type_ids, labels\n",
    "            # 需回傳 tensor 型態，其前三者維度為 torch.Size([self.max_len])，最後者維度為 torch.Size([1])\n",
    "              return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
    "                torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "# 可以先 sample 部分資料去跑模型，有助於快速調整模型架構，畢竟資料愈多跑愈久\n",
    "# 將 Dataset 放入 DataLoader 中，並指定 batch_size\n",
    "train_df = pd.read_csv('./data/train.tsv', sep = '\\t').sample(parameters['train_size'], random_state=parameters['seed']).reset_index(drop=True)\n",
    "train_dataset = CustomDataset('train', train_df, 'text', parameters)\n",
    "train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "# load validation data\n",
    "val_df = pd.read_csv('./data/val.tsv', sep = '\\t').sample(parameters['val_size'], random_state=parameters['seed']).reset_index(drop=True)\n",
    "val_dataset = CustomDataset('val', val_df, 'text', parameters)\n",
    "val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "for data in val_loader:\n",
    "    # pprint(data)\n",
    "    # print(len(data))\n",
    "    ids, masks, token_type_ids, labels = data\n",
    "    print(ids.shape)\n",
    "    print(masks.shape)\n",
    "    print(token_type_ids.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義激活函數\n",
    "def get_activation(activation):\n",
    "    if activation == 'Prelu':\n",
    "        return nn.PReLU()\n",
    "    elif activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif activation == 'LeakyReLU':\n",
    "        return nn.LeakyReLU()\n",
    "    else:\n",
    "        return nn.Tanh()\n",
    "\n",
    "# Dense Layer\n",
    "# It is composed of linear, dropout, and activation layers.\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate, activation='tanh'):\n",
    "        super(Dense, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, output_dim)    # 全連接層，輸入維度是 input_dim，輸出維度是 output_dim\n",
    "        self.dropout = nn.Dropout(dropout_rate)                 # 定義 Dropout 層\n",
    "        self.activation = get_activation(activation)            # 指定激活函數\n",
    "        nn.init.xavier_uniform_(self.hidden_layer.weight)       # Xavier 初始化，有助於模型收斂\n",
    "    def forward(self, inputs):\n",
    "        logits = self.hidden_layer(inputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.activation(logits)\n",
    "        return logits\n",
    "\n",
    "# multi-layers\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Hidden Layers\n",
    "# It means there are many dense layers with the same dimension\n",
    "class HiddenLayers(nn.Module):\n",
    "    def __init__(self, dense_layer, num_layers):\n",
    "        super(HiddenLayers, self).__init__()\n",
    "        self.hidden_layers = _get_clones(dense_layer, num_layers)   # dense_layer 為 Dense 物件，num_layers 為複製幾個 Dense 物件\n",
    "    def forward(self, output):\n",
    "        for layer in self.hidden_layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Model\n",
    "class BertClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        '''\n",
    "        當呼叫 BertClassifier.from_pretrained(parameters['config'], parameters) 時會執行\n",
    "        config: pretrained Model 的參數， config = AutoConfig.from_pretrained(config_name)\n",
    "        args: 自行添加的參數，會將 parameters 傳入\n",
    "        '''\n",
    "        super(BertClassifier, self).__init__(config)\n",
    "        self.bert = BertModel(config)               # 初始化 Bert 模型\n",
    "        self.num_labels = args[\"num_class\"]         # 指定 label 種類數量\n",
    "        self.dense = Dense(config.hidden_size, args[\"hidden_dim\"], args[\"dropout\"], args[\"activation\"])     # 宣告 Dense 作為 fully connected\n",
    "        self.classifier = Dense(args[\"hidden_dim\"], self.num_labels, args[\"dropout\"], args[\"activation\"])   # 宣告 Dense 作為 linear classfier\n",
    "        self.init_weights()                         # 初始化權重\n",
    "    \n",
    "    # forward function, data in the model will do this\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
    "                head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,\n",
    "                output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # 使用原始 BertModel 進行預測\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        # 原始 BERT 模型的回傳:\n",
    "        # outputs.keys() -> odict_keys(['last_hidden_state', 'pooler_output'])\n",
    "        # outputs.last_hidden_state.shape -> torch.Size([batch_size, args[\"max_length\"], config.hidden_size])\n",
    "        # outputs.pooler_output.shape -> torch.Size([batch_size, config.hidden_size])\n",
    "\n",
    "        # pooler_output 維度為 [batch_size, config.hidden_size] ，其實就是 last_hidden_state 的第一個 token ， 即 [CLS] logits\n",
    "        pooled_output = outputs[1]                  # (batch_size, config.hidden_size)\n",
    "\n",
    "        # 加上 Dense ，即將上游任務 Embedding 結果放到下游任務中\n",
    "        pooled_output = self.dense(pooled_output)   # (batch_size, args[\"hidden_dim\"]) 添加一層 NN ，作為正式進去線性分類層的緩衝(先降維度)\n",
    "        logits = self.classifier(pooled_output)     # (batch_size, self.num_labels) 線性分類層\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassifier were not initialized from the model checkpoint at nbroad/ESG-BERT and are newly initialized: ['classifier.hidden_layer.bias', 'classifier.hidden_layer.weight', 'dense.hidden_layer.bias', 'dense.hidden_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # 確認GPU可否使用\n",
    "model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device) # 載入域訓練模型與傳入參數，並放到GPU計算\n",
    "loss_fct = nn.CrossEntropyLoss()    # 使用 cross entrophy loss\n",
    "\n",
    "## You can custom your optimizer (e.g. SGD .etc) ##\n",
    "# we use Adam here\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)\n",
    "\n",
    "## You also can add your custom scheduler ##\n",
    "# num_train_steps = len(train_loader) * parameters['epochs]\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_train_steps), num_training_steps=num_train_steps, num_cycles=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to path\n",
    "def save_checkpoint(save_path, model):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "# load model from path\n",
    "def load_checkpoint(load_path, model, device):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'\\nModel loaded from <== {load_path}')\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(logits):\n",
    "    '''\n",
    "    舉例說明:\n",
    "    輸入為: logits = torch.tensor([\n",
    "      [2.5, 0.3, 1],  # 第 1 個樣本\n",
    "      [1.0, 3.1, 5],  # 第 2 個樣本\n",
    "      [0.2, 0.9, 2.1],  # 第 3 個樣本\n",
    "      [1.5, 1.5, 0.5]   # 第 4 個樣本\n",
    "    ])\n",
    "    輸出為: tensor([0, 2, 2, 0])，維度為 torch.size([batch_size])\n",
    "    '''\n",
    "    y_pred = torch.argmax(logits, dim = 1)      # 從 logits 第一維找出最大值的位置\n",
    "    return y_pred\n",
    "\n",
    "# calculate confusion metrics\n",
    "def cal_metrics(pred, ans, method):\n",
    "    '''\n",
    "    Parameter\n",
    "    ---------\n",
    "    pred: [list], predict class\n",
    "    ans: [list], true class\n",
    "    method: 'micro', 'weighted', 'macro'. # 如果有多分類的話計算上會有差別\n",
    "    'micro'：基於全體樣本計算，計算所有樣本的總體效果。\n",
    "    'macro'：對每個類別分別計算指標，然後取平均，不考慮類別樣本數差異。\n",
    "    'weighted'：對每個類別分別計算指標，然後根據樣本數取加權平均。\n",
    "    ---------\n",
    "    '''\n",
    "    # 將 tensor 移動到 CPU 並轉成 numpy\n",
    "    if pred.get_device() != 'cpu':\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "    if ans.get_device() != 'cpu':\n",
    "        ans = ans.detach().cpu().numpy()\n",
    "    # sklearn.metrics 的各式計算方法須將 pred ans 兩個 label list 放入 \n",
    "    # 將 zero_division 設為 0，表示當所有預測皆錯誤時，將結果視為 0 \n",
    "    rec = recall_score(pred, ans, average=method, zero_division=0)\n",
    "    f1 = f1_score(pred, ans, average=method, zero_division=0)\n",
    "    prec = precision_score(pred, ans, average=method, zero_division=0)\n",
    "    acc = accuracy_score(pred, ans)\n",
    "    return acc, f1, rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate dataloader\n",
    "def evaluate(model, data_loader, device):\n",
    "    val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    step_count = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
    "\n",
    "            logits = model(input_ids = ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    attention_mask = masks)\n",
    "            \n",
    "            acc, f1, rec, prec = cal_metrics(get_class(logits), get_class(labels), 'weighted')\n",
    "            loss = loss_fct(logits, get_class(labels))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "            val_f1 += f1\n",
    "            val_rec += rec\n",
    "            val_prec += prec\n",
    "            step_count+=1\n",
    "\n",
    "        val_loss = val_loss / step_count\n",
    "        val_acc = val_acc / step_count\n",
    "        val_f1 = val_f1 / step_count\n",
    "        val_rec = val_rec / step_count\n",
    "        val_prec = val_prec / step_count\n",
    "\n",
    "    return val_loss, val_acc, val_f1, val_rec, val_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, args, device):\n",
    "    time_stamp = str(datetime.now().strftime(\"%m-%d-%H-%M\"))\n",
    "    metrics = ['loss', 'acc', 'f1', 'rec', 'prec']\n",
    "    mode = ['train_', 'val_']\n",
    "    record = {s+m :[] for s in mode for m in metrics}\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "\n",
    "        st_time = time.time()\n",
    "        train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        step_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            # labels 維度為 torch.size([batch_size, args[num_class]])\n",
    "            ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # logits 維度為 torch.size([batch_size, args[num_class]])\n",
    "            logits = model(input_ids = ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    attention_mask = masks)\n",
    "\n",
    "            acc, f1, rec, prec = cal_metrics(get_class(logits), get_class(labels), 'weighted')\n",
    "\n",
    "            # CrossEntropy 計算\n",
    "            # pred 必須是 logits， 會自動轉成 softmax\n",
    "            # labels 必須是類別\n",
    "            loss = loss_fct(logits, get_class(labels))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc\n",
    "            train_f1 += f1\n",
    "            train_rec += rec\n",
    "            train_prec += prec\n",
    "            step_count += 1\n",
    "\n",
    "        val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)\n",
    "\n",
    "        train_loss = train_loss / step_count\n",
    "        train_acc = train_acc / step_count\n",
    "        train_f1 = train_f1 / step_count\n",
    "        train_rec = train_rec / step_count\n",
    "        train_prec = train_prec / step_count\n",
    "\n",
    "        print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))\n",
    "        print('         loss     acc     f1      rec    prec')\n",
    "        print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))\n",
    "        print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))\n",
    "\n",
    "        # record training metrics of each training epoch\n",
    "        record['train_loss'].append(train_loss)\n",
    "        record['train_acc'].append(train_acc)\n",
    "        record['train_f1'].append(train_f1)\n",
    "        record['train_rec'].append(train_rec)\n",
    "        record['train_prec'].append(train_prec)\n",
    "    \n",
    "        record['val_loss'].append(val_loss)\n",
    "        record['val_acc'].append(val_acc)\n",
    "        record['val_f1'].append(val_f1)\n",
    "        record['val_rec'].append(val_rec)\n",
    "        record['val_prec'].append(val_prec)\n",
    "\n",
    "        # save model\n",
    "        model_filename = f\"./model/{time_stamp}_epoch{epoch}.pt\"\n",
    "        save_checkpoint(model_filename, model)\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# draw the learning curve\n",
    "def draw_pic(record, name, img_save=False, show=False):\n",
    "    x_ticks = range(1, parameters['epochs']+1)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue',\n",
    "             markeredgecolor=\"teal\", markersize=3, markeredgewidth=1, label = 'Train')\n",
    "    plt.plot(x_ticks, record['val_'+name], '-o', color='pink',\n",
    "             markeredgecolor=\"salmon\", markersize=3, markeredgewidth=1, label = 'Val')\n",
    "    plt.grid(color='lightgray', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title('Model', fontsize=14)\n",
    "    plt.ylabel(name, fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.xticks(x_ticks, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')\n",
    "\n",
    "    # define saved figure or not\n",
    "    if img_save:\n",
    "        plt.savefig(f'result_img/{name}.png', transparent=False, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] cost time: 32.8882 s\n",
      "         loss     acc     f1      rec    prec\n",
      "train | 0.6806, 0.8519, 0.8642, 0.8519, 0.8950\n",
      "val  | 0.4765, 0.9342, 0.9426, 0.9342, 0.9567\n",
      "\n",
      "Model saved to ==> ./model/10-28-19-03_epoch0.pt\n",
      "[epoch 2] cost time: 32.6644 s\n",
      "         loss     acc     f1      rec    prec\n",
      "train | 0.5284, 0.9311, 0.9313, 0.9311, 0.9381\n",
      "val  | 0.5421, 0.8962, 0.8934, 0.8962, 0.9093\n",
      "\n",
      "Model saved to ==> ./model/10-28-19-03_epoch1.pt\n",
      "[epoch 3] cost time: 32.5768 s\n",
      "         loss     acc     f1      rec    prec\n",
      "train | 0.5153, 0.9327, 0.9319, 0.9327, 0.9394\n",
      "val  | 0.5461, 0.8733, 0.8647, 0.8733, 0.8942\n",
      "\n",
      "Model saved to ==> ./model/10-28-19-03_epoch2.pt\n"
     ]
    }
   ],
   "source": [
    "history = train(model, train_loader, val_loader, optimizer, parameters, device)\n",
    "\n",
    "# draw all metrics figure\n",
    "draw_pic(history, 'loss', img_save=True, show=False)\n",
    "draw_pic(history, 'acc', img_save=True, show=False)\n",
    "draw_pic(history, 'f1', img_save=True, show=False)\n",
    "draw_pic(history, 'rec', img_save=True, show=False)\n",
    "draw_pic(history, 'prec', img_save=True, show=False)\n",
    "\n",
    "files = []\n",
    "files.append('result_img/loss.png')\n",
    "files.append('result_img/acc.png')\n",
    "files.append('result_img/f1.png')\n",
    "files.append('result_img/rec.png')\n",
    "files.append('result_img/prec.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()\n",
    "\n",
    "# predict a single sentence\n",
    "def predict_one(query, model):\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            query,\n",
    "            max_length = parameters['max_len'],\n",
    "            truncation = True,\n",
    "            padding = 'max_length',\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(input_ids, attention_mask, token_type_ids)\n",
    "    probs = Softmax(logits) # get each class-probs\n",
    "    label_index = torch.argmax(probs[0], dim=0)\n",
    "    pred = label_index.item()\n",
    "\n",
    "  return probs, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassifier were not initialized from the model checkpoint at nbroad/ESG-BERT and are newly initialized: ['classifier.hidden_layer.bias', 'classifier.hidden_layer.weight', 'dense.hidden_layer.bias', 'dense.hidden_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\simsIab\\AppData\\Local\\Temp\\ipykernel_10236\\1987939282.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded from <== ./model/10-28-19-03_epoch0.pt\n"
     ]
    }
   ],
   "source": [
    "# You can load the model from the existing result\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "init_model = BertClassifier.from_pretrained(parameters['config'], parameters) # build an initial model\n",
    "model = load_checkpoint('./model/10-28-19-03_epoch0.pt', init_model, device).to(device) # and load the weight of model from specify file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1132, 0.1009, 0.1001, 0.6859]], device='cuda:0') 3\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = \"Completely redesigned forn the first time since its debut 2007 model year, the 2016 Lincoln MKX is equipped with a number of advanced safety and driver assist technologies.\"\n",
    "probs, pred = predict_one(test, model)\n",
    "print(probs, pred)\n",
    "\n",
    "# '''\n",
    "# tensor([[0.9779, 0.0221]], device='cuda:0') 0\n",
    "# CPU times: user 78.1 ms, sys: 4 ms, total: 82.1 ms\n",
    "# Wall time: 340 ms\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         loss     acc     f1      rec    prec\n",
      "test  | 0.4760, 0.9346, 0.9425, 0.9346, 0.9567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 資料測試\n",
    "test_df = pd.read_csv('./data/test.tsv', sep = '\\t').reset_index(drop=True)\n",
    "test_dataset = CustomDataset('val', val_df, 'text', parameters)\n",
    "test_loader = DataLoader(test_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
    "test_loss, test_acc, test_f1, test_rec, test_prec = evaluate(model, test_loader, device)\n",
    "print('         loss     acc     f1      rec    prec')\n",
    "print('test  | %.4f, %.4f, %.4f, %.4f, %.4f\\n'%(test_loss, test_acc, test_f1, test_rec, test_prec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertClassfication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
